# ğŸ“‹ RESUMEN COMPLETO DEL PROYECTO - Estado Actual y PrÃ³ximos Pasos

## âœ… LO QUE YA ESTÃ COMPLETADO

### **1. Infraestructura Base**
```
âœ… Servidor: 64GB RAM, Ubuntu 22.04
âœ… PostgreSQL 15 instalado
âœ… TimescaleDB 2.23.0 configurado
âœ… Python 3.10 + uv como gestor de paquetes
âœ… Dependencias instaladas: pandas, pyarrow, psycopg2, sqlalchemy, pyyaml, tqdm
```

### **2. Arquitectura HÃ­brida Implementada**

```
PARQUET (Almacenamiento histÃ³rico)
â”œâ”€ UbicaciÃ³n: /dados/avc/parquet/
â”œâ”€ Archivo principal: transacciones_consolidadas.parquet
â”œâ”€ Registros: 227,233,140 transacciones (2 aÃ±os completos)
â”œâ”€ PerÃ­odo: 2024-2025
â”œâ”€ Meses: 14 meses consolidados
â”œâ”€ TamaÃ±o: ~5 GB
â””â”€ Uso: Entrenamiento de modelos ML, anÃ¡lisis histÃ³ricos

POSTGRESQL + TIMESCALEDB (Base operacional)
â”œâ”€ Base de datos: fraud_detection
â”œâ”€ Usuario: fraud_user
â”œâ”€ Registros: ~13-14M transacciones (Ãºltimos 6 meses)
â”œâ”€ Filtros aplicados:
â”‚   â”œâ”€ Solo Ãºltimos 6 meses
â”‚   â”œâ”€ Tipo OperaciÃ³n: Cambio De Pin, Avance, Retiro, Depositos, Transferencias
â”‚   â”œâ”€ Autorizador requerido (no NULL)
â”‚   â””â”€ Sin duplicados
â”œâ”€ CompresiÃ³n: Columnstore habilitado (ahorro 50-70% espacio)
â””â”€ Uso: Dashboard en tiempo real, queries rÃ¡pidas (<1 seg)
```

### **3. Estructura de Base de Datos**

```sql
fraud_detection
â”œâ”€â”€ cajeros (metadata de ATMs)
â”‚   â”œâ”€â”€ 25 columnas (codigo, longitud, latitud, municipio, etc.)
â”‚   â”œâ”€â”€ Ãndices: ubicaciÃ³n, tipo, estado
â”‚   â””â”€â”€ Estado: âš ï¸ VACÃA (archivo Excel pendiente de cargar)
â”‚
â”œâ”€â”€ transacciones (hypertable con TimescaleDB)
â”‚   â”œâ”€â”€ 20 columnas + fecha_transaccion_15min (granularidad 15 min)
â”‚   â”œâ”€â”€ ~13-14M registros
â”‚   â”œâ”€â”€ Particionado por semanas (chunk_interval: 1 week)
â”‚   â”œâ”€â”€ CompresiÃ³n automÃ¡tica despuÃ©s de 30 dÃ­as
â”‚   â””â”€â”€ Ãndices optimizados
â”‚
â””â”€â”€ Tablas preparadas (vacÃ­as, para prÃ³ximas fases):
    â”œâ”€â”€ features (para features calculadas)
    â”œâ”€â”€ scores (para anomalÃ­as detectadas)
    â”œâ”€â”€ razones_anomalias (explicaciones detalladas)
    â”œâ”€â”€ feedback (validaciÃ³n humana - Fase 2)
    â””â”€â”€ modelos (versionamiento de modelos)
```

### **4. Scripts de ProducciÃ³n Creados**

```
/dados/avc/
â”œâ”€â”€ config.yaml (configuraciÃ³n centralizada)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ consolidar_a_parquet.py âœ… (CSV â†’ Parquet)
â”‚   â””â”€â”€ cargar_a_postgres.py âœ… (Parquet â†’ PostgreSQL)
â””â”€â”€ logs/
    â”œâ”€â”€ consolidacion.log
    â””â”€â”€ postgres.log
```

### **5. Modelos Desarrollados (Fase 1 previa)**

```
âœ… Modelo 1: Reglas de Negocio (6 reglas detectando patrones conocidos)
âœ… Modelo 2: Isolation Forest (detecciÃ³n no supervisada)
```

---

## ğŸ¯ ESTADO ACTUAL - DÃ“NDE ESTAMOS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ROADMAP COMPLETO                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FASE 1: PreparaciÃ³n y EDA                          âœ… 100% â”‚
â”‚ FASE 2: Feature Engineering                        â³ 0%   â”‚
â”‚ FASE 3: Modelado ML                                â³ 40%  â”‚
â”‚ FASE 4: Dashboard Interactivo                      â³ 0%   â”‚
â”‚ FASE 5: IntegraciÃ³n y PresentaciÃ³n                 â³ 0%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Estamos entre FASE 1 y FASE 2:**
- âœ… Datos consolidados y cargados
- âœ… Infraestructura lista
- â³ Falta: Calcular features y aplicar modelos sobre PostgreSQL

---

## ğŸ“Š DATOS DISPONIBLES

### **Transacciones en PostgreSQL:**
```
- Total registros: ~13-14M
- PerÃ­odo: Ãšltimos 6 meses
- Tipos de operaciÃ³n: 5 tipos relevantes
- Granularidad temporal: 15 minutos (fecha_transaccion_15min)
- Sin duplicados
- Todos con autorizador vÃ¡lido
```

### **Columnas principales en transacciones:**
```
- id_tlf (identificador transacciÃ³n)
- fecha_transaccion (timestamp original)
- fecha_transaccion_15min (redondeado a 15 min)
- cod_terminal (cÃ³digo del cajero)
- tipo_operacion (Avance, Retiro, etc.)
- valor_transaccion (monto)
- cod_estado_transaccion (1=exitosa, 2=rechazada)
- autorizador (banco)
- adquiriente
- archivo_origen, mes_origen
```

---

## ğŸš€ PRÃ“XIMOS PASOS - LO QUE FALTA HACER

### **PASO 1: Cargar Metadata de Cajeros** âš ï¸ PENDIENTE

**Problema actual:** Tabla `cajeros` estÃ¡ vacÃ­a

**SoluciÃ³n:**

1. **Limpiar archivo Excel:**
   ```bash
   # Crear script para limpiar Excel
   cd /dados/avc
   nano scripts/limpiar_excel_cajeros.py
   ```

   ```python
   import pandas as pd
   
   # Leer desde lÃ­nea 3 (donde estÃ¡n los headers)
   df = pd.read_excel(
       "data/Inventario General Disp ATM_Centro de Efectivo_36_8260511562052405324.xlsx", 
       header=3
   )
   
   # Eliminar primera columna vacÃ­a
   df = df.iloc[:, 1:]
   
   # Guardar limpio
   df.to_excel("Inventario_General_Disp_ATM_Centro_de_Efectivo.xlsx", index=False)
   print(f"âœ… {len(df):,} cajeros exportados")
   ```

2. **Ejecutar limpieza:**
   ```bash
   uv run scripts/limpiar_excel_cajeros.py
   ```

3. **Cargar a PostgreSQL:**
   ```bash
   # OpciÃ³n A: Re-ejecutar cargar_a_postgres.py completo
   uv run scripts/cargar_a_postgres.py --config config.yaml
   
   # OpciÃ³n B: Script especÃ­fico para solo cajeros (crear)
   ```

**Tiempo estimado:** 5-10 minutos

---

### **PASO 2: Feature Engineering (CRÃTICO)** ğŸ”¥

**Objetivo:** Calcular 25+ features para ML desde tabla `transacciones`

**Script a crear:** `/dados/avc/scripts/calcular_features.py`

**Features a calcular (del Roadmap):**

#### **A. Features Temporales:**
```python
- hora (0-23)
- dia_semana (0-6)
- es_fin_de_semana (bool)
- es_horario_nocturno (bool: 22:00-06:00)
- es_madrugada (bool: 00:00-06:00)
```

#### **B. Features de Monto:**
```python
- diferencia_valor (Valor TransacciÃ³n - Valor Original)
- es_retiro_maximo (bool: >=2,000,000)
- monto_normalizado_por_cajero (z-score)
```

#### **C. Features de Velocidad:**
```python
- tiempo_desde_anterior_seg (tiempo entre tx en mismo cajero)
- es_transaccion_rapida (bool: <10 seg)
- velocidad_tx_por_minuto (tx/min por cajero)
```

#### **D. Features de OperaciÃ³n:**
```python
- es_cambio_pin (bool)
- tipo_operacion_encoded (numÃ©rico)
- transaccion_exitosa (estado=1)
- transaccion_rechazada (estado=2)
```

#### **E. Features Agregadas por Cajero:**
```python
- tx_por_hora_cajero (rolling window 1 hora)
- monto_promedio_cajero (histÃ³rico)
- tasa_rechazo_cajero (rechazadas/total)
- desviacion_monto_cajero (std dev)
- velocidad_promedio_cajero
```

#### **F. Features de Cajero (desde metadata):**
```python
- cajero_adyacente_encoded (boolâ†’int)
- cierre_nocturno_encoded (boolâ†’int)
- es_ubicacion_aislada
- tipo_funcion_encoded
```

**ImplementaciÃ³n:**

```python
# PseudocÃ³digo de calcular_features.py

import pandas as pd
from sqlalchemy import create_engine
import yaml

# 1. Conectar a PostgreSQL
# 2. Leer transacciones
# 3. Calcular features temporales (pandas)
# 4. Calcular features de ventana mÃ³vil (groupby + rolling)
# 5. JOIN con cajeros para features de ubicaciÃ³n
# 6. Guardar en tabla features
# 7. Log de progreso
```

**Tiempo estimado:** 2-3 horas de desarrollo + 10-20 min de ejecuciÃ³n

---

### **PASO 3: Aplicar Modelos de DetecciÃ³n** ğŸ¤–

**Scripts a adaptar:**
- `MODELO_1_REGLAS_NEGOCIO.py` (ya existe, adaptar a PostgreSQL)
- `MODELO_2_ISOLATION_FOREST.py` (ya existe, adaptar a PostgreSQL)

**Proceso:**

1. **Leer features desde PostgreSQL**
2. **Aplicar Modelo 1 (Reglas):**
   ```python
   # 6 reglas hardcodeadas:
   - MÃ¡s de 5 cambios PIN en 1 hora â†’ CRÃTICO
   - Retiros > $2M en 10 minutos â†’ CRÃTICO
   - 10+ rechazos consecutivos â†’ ADVERTENCIA
   - Transacciones fuera de horario â†’ ADVERTENCIA
   - Velocidad > 10 tx/minuto â†’ CRÃTICO
   - PatrÃ³n PINâ†’Retiro < 5 min â†’ CRÃTICO
   ```

3. **Aplicar Modelo 2 (Isolation Forest):**
   ```python
   # Cargar modelo entrenado (.pkl)
   # Predecir anomalÃ­as
   # Score de 0-1
   ```

4. **Combinar scores:**
   ```python
   score_final = 0.5 * score_reglas + 0.5 * score_isolation_forest
   
   if score_final > 0.8: nivel = 'CRÃTICO'
   elif score_final > 0.5: nivel = 'SOSPECHOSO'
   else: nivel = 'NORMAL'
   ```

5. **Guardar en tabla `scores`:**
   ```sql
   INSERT INTO scores (
       id_transaccion,
       score_reglas,
       score_isolation_forest,
       score_final,
       nivel_anomalia,
       fecha_scoring
   ) VALUES (...)
   ```

**Tiempo estimado:** 1-2 horas de adaptaciÃ³n + 5-10 min de ejecuciÃ³n

---

### **PASO 4: Generar Razones Detalladas** ğŸ“

**Script a crear:** `generar_razones_anomalias.py`

**Objetivo:** Explicar POR QUÃ‰ cada transacciÃ³n es anÃ³mala

**Ejemplo de razones:**
```python
TransacciÃ³n ID: 123456789
Score Final: 0.92 (CRÃTICO)

Razones:
1. Monto anÃ³malo: $2,500,000 (3.5Ïƒ sobre promedio del cajero)
2. Velocidad alta: 12 tx/minuto (normal: 2 tx/min)
3. Horario sospechoso: 03:45 AM (cajero cierra a 22:00)
4. PatrÃ³n Cambio PIN â†’ Retiro en 3 minutos
5. UbicaciÃ³n de riesgo: Cajero aislado sin adyacencia
```

**ImplementaciÃ³n:**
```python
# Para cada anomalÃ­a en scores:
for transaccion in anomalias:
    razones = []
    
    # Analizar cada feature
    if monto > umbral_monto:
        razones.append(("Monto", "Alto", f"{monto:,.0f} > {umbral:,.0f}"))
    
    if velocidad > umbral_velocidad:
        razones.append(("Velocidad", "Alta", f"{velocidad} tx/min"))
    
    # ... mÃ¡s checks
    
    # Guardar en razones_anomalias
    INSERT INTO razones_anomalias (id_transaccion, tipo_razon, detalle, severidad)
```

**Tiempo estimado:** 1 hora de desarrollo + 5 min de ejecuciÃ³n

---

### **PASO 5: Dashboard BÃ¡sico (MVP)** ğŸ“Š

**Herramienta:** Streamlit (Python) o conectar a Power BI

**Componentes mÃ­nimos:**

```python
# dashboard.py con Streamlit

import streamlit as st
import pandas as pd
import plotly.express as px
from sqlalchemy import create_engine

st.title("ğŸš¨ Sistema de DetecciÃ³n de Fraudes ATM")

# KPIs principales
col1, col2, col3 = st.columns(3)
col1.metric("Alertas CrÃ­ticas", num_criticas, delta="+5")
col2.metric("Alertas Sospechosas", num_sospechosas)
col3.metric("Cajeros Monitoreados", num_cajeros)

# Mapa de cajeros con alertas
fig_mapa = px.scatter_mapbox(
    df_alertas,
    lat="latitud",
    lon="longitud",
    color="nivel_anomalia",
    hover_data=["cod_terminal", "score_final"],
    mapbox_style="open-street-map"
)
st.plotly_chart(fig_mapa)

# Timeline de alertas
fig_timeline = px.bar(
    df_timeline,
    x="fecha_transaccion_15min",
    y="num_alertas",
    color="nivel_anomalia"
)
st.plotly_chart(fig_timeline)

# Tabla de alertas detalladas
st.dataframe(df_alertas_detalle)
```

**Ejecutar:**
```bash
uv pip install streamlit plotly
streamlit run dashboard.py
```

**Tiempo estimado:** 2-3 horas de desarrollo

---

### **PASO 6: Reportes AutomÃ¡ticos por Email** ğŸ“§

**Script a crear:** `reportes_email.py`

**Objetivo:** Enviar reporte cada 15 minutos si hay alertas

**Componentes:**
```python
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from jinja2 import Template

# Template HTML del reporte
template = """
<html>
<body>
    <h2>ğŸš¨ Reporte de Alertas - {{ fecha }}</h2>
    
    <h3>Resumen:</h3>
    <ul>
        <li>Alertas CrÃ­ticas: {{ num_criticas }}</li>
        <li>Alertas Sospechosas: {{ num_sospechosas }}</li>
    </ul>
    
    <h3>Top 5 Cajeros con Alertas:</h3>
    <table>
        {% for cajero in top_cajeros %}
        <tr>
            <td>{{ cajero.codigo }}</td>
            <td>{{ cajero.ubicacion }}</td>
            <td>{{ cajero.num_alertas }} alertas</td>
        </tr>
        {% endfor %}
    </table>
</body>
</html>
"""

# Generar y enviar
html = template.render(fecha=now, num_criticas=..., ...)
enviar_email(destinatarios, "Reporte Fraudes ATM", html)
```

**Configurar cron:**
```bash
# Ejecutar cada 15 minutos
*/15 * * * * cd /dados/avc && /dados/avc/.venv/bin/python scripts/reportes_email.py
```

**Tiempo estimado:** 1-2 horas de desarrollo

---

## ğŸ“… CRONOGRAMA SUGERIDO

### **Semana 1: Feature Engineering**
```
Lunes:    Cargar metadata cajeros
Martes:   Desarrollar script calcular_features.py (features temporales)
MiÃ©rcoles: Continuar features (agregadas por cajero)
Jueves:   Continuar features (joins con cajeros)
Viernes:  Ejecutar y validar features completas
```

### **Semana 2: Modelos y DetecciÃ³n**
```
Lunes:    Adaptar Modelo 1 (Reglas) a PostgreSQL
Martes:   Adaptar Modelo 2 (Isolation Forest) a PostgreSQL
MiÃ©rcoles: Aplicar modelos y generar scores
Jueves:   Desarrollar script razones_anomalias.py
Viernes:  Validar detecciones con casos reales
```

### **Semana 3: Dashboard y Reportes**
```
Lunes-Martes:    Desarrollar dashboard Streamlit
MiÃ©rcoles:       Desarrollar reportes email
Jueves:          Testing end-to-end
Viernes:         PresentaciÃ³n al cliente
```

---

## ğŸ”§ COMANDOS ÃšTILES PARA CONTINUAR

### **Conectar a PostgreSQL:**
```bash
psql -U fraud_user -d fraud_detection
```

### **Queries de verificaciÃ³n:**
```sql
-- Ver registros en transacciones
SELECT COUNT(*) FROM transacciones;

-- Ver tipos de operaciÃ³n
SELECT tipo_operacion, COUNT(*) 
FROM transacciones 
GROUP BY tipo_operacion;

-- Ver rango de fechas
SELECT MIN(fecha_transaccion), MAX(fecha_transaccion) 
FROM transacciones;

-- Ver cajeros (deberÃ­a estar vacÃ­a por ahora)
SELECT COUNT(*) FROM cajeros;

-- Ver estructura de features (vacÃ­a)
SELECT * FROM features LIMIT 1;
```

### **Activar entorno Python:**
```bash
cd /dados/avc
source .venv/bin/activate  # o usar: uv run
```

### **Ver logs:**
```bash
tail -f /dados/avc/logs/postgres.log
tail -f /dados/avc/logs/consolidacion.log
```

---

## ğŸ“‚ ESTRUCTURA DE ARCHIVOS ACTUALIZADA

```
/dados/avc/
â”œâ”€â”€ config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 2024/ (CSVs originales)
â”‚   â”œâ”€â”€ 2025/ (CSVs originales)
â”‚   â””â”€â”€ Inventario General... .xlsx (metadata cajeros)
â”‚
â”œâ”€â”€ parquet/
â”‚   â”œâ”€â”€ enero_2024.parquet ... junio_2025.parquet (14 archivos)
â”‚   â””â”€â”€ transacciones_consolidadas.parquet (227M registros)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ consolidar_a_parquet.py âœ…
â”‚   â”œâ”€â”€ cargar_a_postgres.py âœ…
â”‚   â”œâ”€â”€ limpiar_excel_cajeros.py (por crear)
â”‚   â”œâ”€â”€ calcular_features.py (por crear)
â”‚   â”œâ”€â”€ aplicar_modelos.py (por crear)
â”‚   â”œâ”€â”€ generar_razones.py (por crear)
â”‚   â”œâ”€â”€ dashboard.py (por crear)
â”‚   â””â”€â”€ reportes_email.py (por crear)
â”‚
â”œâ”€â”€ models/ (modelos entrenados .pkl - ya tienes algunos)
â”œâ”€â”€ logs/ (consolidacion.log, postgres.log)
â””â”€â”€ outputs/ (reportes, mapas generados)
```

---

## âš ï¸ PUNTOS CRÃTICOS A RECORDAR

1. **Tabla cajeros vacÃ­a** - Cargar primero antes de calcular features que la usen
2. **Granularidad 15 minutos** - Usar `fecha_transaccion_15min` para agregaciones
3. **Filtros aplicados** - PostgreSQL tiene solo datos filtrados (no todo el histÃ³rico)
4. **CompresiÃ³n activa** - Chunks de +30 dÃ­as se comprimen automÃ¡ticamente
5. **Parquet intacto** - Para reentrenar modelos usa Parquet, no PostgreSQL

---

## ğŸ¯ OBJETIVO FINAL

```
Sistema en producciÃ³n con:
âœ… 227M transacciones histÃ³ricas (Parquet)
âœ… 14M transacciones operacionales (PostgreSQL Ãºltimos 6 meses)
âœ… Features calculadas para todas las transacciones
âœ… 2 modelos detectando anomalÃ­as
âœ… Scores y razones detalladas
âœ… Dashboard en tiempo real
âœ… Reportes automÃ¡ticos cada 15 minutos
âœ… Metadata de cajeros integrada
â³ (Fase 2 futura: Supervised Learning con feedback)
```

---

## ğŸ“ INFORMACIÃ“N DE CONTACTO DEL SISTEMA

```
Servidor: soacolpoc01
Usuario: jmcardenas1
Directorio: /dados/avc/
Base de datos: fraud_detection
Usuario DB: fraud_user
Puerto: 5432
```

---
